{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, save word embeddings from doc2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import os\n",
    "\n",
    "base_dirpath = '/usr0/home/mamille2/erebor/'\n",
    "\n",
    "model_path = os.path.join(base_dirpath, 'fanfiction-project', 'models', 'academia-detroit-friends', 'PV-DBOW_d100n5mc2t20.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(os.path.join(base_dirpath, 'word_embeddings', 'academia-detroit-friends_embeddings.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare AO3 data in format for paragraph vector training\n",
    "\n",
    "Preprocess content (tokenized, lowercased, like aclImdb/alldata-id.txt), extact relationship type metadata as tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['annotations', 'content', 'contributionId', 'contributor',\n",
      "       'discoursePartIds', 'discourseParts', 'parentId', 'startTime', 'title',\n",
      "       'type', 'preceding_user', 'preceding_text'],\n",
      "      dtype='object')\n",
      "74199\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load in data from DiscourseDB\n",
    "data = pd.read_csv('/usr2/mamille2/fanfiction-project/data/ao3/friends/friends_discoursedb_data.csv')\n",
    "print(data.columns)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return ' '.join([tok.text for tok in nlp.tokenizer(text.lower().replace('\\n', ' '))]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfed0c8a54a846f697f99590419a0cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=74199), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        \" so , what do we do with it , benny ? \" ray a...\n",
       "1        \" well , i 'm sure someone has noticed that it...\n",
       "2        \" like the dinosaur it came from , for one . \"...\n",
       "3        fraser studied the bone carefully for a moment...\n",
       "4        \" you know this ? \" ray was , as usual , amaze...\n",
       "5        \" note the prismoid shaft which tapers gradual...\n",
       "6        \" obvious to you , maybe . so what 'd your guy...\n",
       "7                       \" metatarsal , \" benny corrected .\n",
       "8        \" okay , metatarsal . what 'd sinclair want wi...\n",
       "9                                  \" how should i know ? \"\n",
       "10       ray sighed , exasperated . \" well , i thought ...\n",
       "11       \" do n't be silly , ray . the inuit have somet...\n",
       "12       \" can we focus here , benny ? this is not the ...\n",
       "13       \" i really do n't know what he wanted with it ...\n",
       "14       \" good luck ! from the looks of this room he '...\n",
       "15       \" good news , benny , \" ray said as his friend...\n",
       "16       \" very funny , ray . \" after two years , frase...\n",
       "17       \" the american museum of natural history in ne...\n",
       "18       ray smiled to himself , holding back the best ...\n",
       "19       \" what was the disturbance ? it must 've been ...\n",
       "20       bingo , ray thought . \" apparently one of the ...\n",
       "21       \" ray , what are you talking about ? are they ...\n",
       "22       \" nope , we 're going to new york . \" ray snuc...\n",
       "23       \" i ca n't go with you , ray . i have to work ...\n",
       "24       \" i called the museum 's board of directors an...\n",
       "25       the flight was a pleasant one with ray pointin...\n",
       "26       \" i did n't realize that there was a place mor...\n",
       "27                                        \" none taken . \"\n",
       "28       ray was enjoying watching fraser out of his el...\n",
       "29       traffic was a nightmare but ray was completely...\n",
       "                               ...                        \n",
       "74169    \" i swear , joey , i never did anything to him...\n",
       "74170    \" what ? i got somethin' stuck in my teeth aga...\n",
       "74171    he had to look at her now . there was pain in ...\n",
       "74172    \" no , joey . it just .. it needs something .....\n",
       "74173                                \" what did you do ? \"\n",
       "74174                             joey hung on , bemused .\n",
       "74175    rachel swallowed hard . \" i could n't move . i...\n",
       "74176    rachel returned with a pair of empty glasses f...\n",
       "74177    joey noticed something now , a faint anomaly o...\n",
       "74178    \" you do n't wear glasses , rachel . besides ,...\n",
       "74179    \" did ross put hands on you ? did he hurt you ...\n",
       "74180    she made a slight smile . \" yeah , i know . it...\n",
       "74181    in her grief and confusion , rachel had almost...\n",
       "74182    joey tried not to let his mind wander down tha...\n",
       "74183    joey 's gut clenched . he had an idea of where...\n",
       "74184    emma was already donning her cutest romper , a...\n",
       "74185    the other part of him - the part he 'd fought ...\n",
       "74186            \" okay . i think you 're ready , joey . \"\n",
       "74187    \" rachel .. i believe you . \" his words were s...\n",
       "74188    \" you sure you 're up to coming with me ? \" he...\n",
       "74189                \" joey , i ca n't ; i mean i just - \"\n",
       "74190    \" of course . i 'm sooo excited for you ! we b...\n",
       "74191    \" trust me . i 'm your friend . everything is ...\n",
       "74192    \" oooey , \" she burbled , her best approximati...\n",
       "74193    she told him - how there were no visible remai...\n",
       "74194    \" see ? alright , i 'm going to get emma 's ca...\n",
       "74195    \" joey , there 's one thing i have n't brought...\n",
       "74196                   \" we are n't taking the subway ? \"\n",
       "74197    he knew . he knew , and yet he resisted . \" wh...\n",
       "74198    rachel smiled . \" i thought you should arrive ...\n",
       "Name: preprocessed_content, Length: 74199, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "data['preprocessed_content'] = list(map(preprocess_text, tqdm(data['content'])))\n",
    "data['preprocessed_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save out documents to their own lines (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paragraphs to their own lines in file\n",
    "with open('/usr2/mamille2/fanfiction-project/data/ao3/friends/friends_paragraphs.txt', 'wb') as f:\n",
    "    for p in data['preprocessed_content'].tolist():\n",
    "        f.write(f'{p}\\n'.encode('utf8'))\n",
    "#         f.write('aééé'.encode('utf8').decode('utf8'))\n",
    "#         f.write('aééé'.encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "data['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# from IPython.core.debugger import set_trace\n",
    "\n",
    "# Extract relationship type column\n",
    "def extract_annotation_category(category, annotations):\n",
    "    annotations_dict = {el[0]: ''.join(el[1:]) for el in [b.split(' (') for b in annotations.split(', ')]}\n",
    "    try:\n",
    "        vals = re.findall(r'null=(.*?)[;\\)]', annotations_dict[category])\n",
    "    except:\n",
    "        set_trace()\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13f4017980042faa39e07b9720635b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=74199), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [F/M, M/M]\n",
       "1        [F/M, M/M]\n",
       "2        [F/M, M/M]\n",
       "3        [M/M, F/M]\n",
       "4        [M/M, F/M]\n",
       "5        [F/M, M/M]\n",
       "6        [M/M, F/M]\n",
       "7        [M/M, F/M]\n",
       "8        [M/M, F/M]\n",
       "9        [M/M, F/M]\n",
       "10       [M/M, F/M]\n",
       "11       [M/M, F/M]\n",
       "12       [M/M, F/M]\n",
       "13       [F/M, M/M]\n",
       "14       [M/M, F/M]\n",
       "15       [M/M, F/M]\n",
       "16       [M/M, F/M]\n",
       "17       [M/M, F/M]\n",
       "18       [M/M, F/M]\n",
       "19       [M/M, F/M]\n",
       "20       [M/M, F/M]\n",
       "21       [F/M, M/M]\n",
       "22       [M/M, F/M]\n",
       "23       [M/M, F/M]\n",
       "24       [M/M, F/M]\n",
       "25       [M/M, F/M]\n",
       "26       [M/M, F/M]\n",
       "27       [M/M, F/M]\n",
       "28       [M/M, F/M]\n",
       "29       [F/M, M/M]\n",
       "            ...    \n",
       "74169         [F/M]\n",
       "74170         [F/M]\n",
       "74171         [F/M]\n",
       "74172         [F/M]\n",
       "74173         [F/M]\n",
       "74174         [F/M]\n",
       "74175         [F/M]\n",
       "74176         [F/M]\n",
       "74177         [F/M]\n",
       "74178         [F/M]\n",
       "74179         [F/M]\n",
       "74180         [F/M]\n",
       "74181         [F/M]\n",
       "74182         [F/M]\n",
       "74183         [F/M]\n",
       "74184         [F/M]\n",
       "74185         [F/M]\n",
       "74186         [F/M]\n",
       "74187         [F/M]\n",
       "74188         [F/M]\n",
       "74189         [F/M]\n",
       "74190         [F/M]\n",
       "74191         [F/M]\n",
       "74192         [F/M]\n",
       "74193         [F/M]\n",
       "74194         [F/M]\n",
       "74195         [F/M]\n",
       "74196         [F/M]\n",
       "74197         [F/M]\n",
       "74198         [F/M]\n",
       "Name: relationship_type, Length: 74199, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['relationship_type'] = [extract_annotation_category('category', x) for x in tqdm(data['annotations'].tolist())]\n",
    "data['relationship_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save out annotations, preprocessed content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '/usr2/mamille2/fanfiction-project/data/ao3/friends/friends_discoursedb_data.pkl'\n",
    "data.to_pickle(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2Vec with AO3 data (documents tagged with relationship type)\n",
    "From https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74199\n",
      "CPU times: user 776 ms, sys: 148 ms, total: 924 ms\n",
      "Wall time: 927 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from smart_open import smart_open\n",
    "\n",
    "fpath = '/usr2/mamille2/fanfiction-project/data/ao3/friends/friends_discoursedb_data.pkl'\n",
    "test_fraction = 0.1\n",
    "\n",
    "data = pd.read_pickle(fpath)\n",
    "total_docs = len(data)\n",
    "\n",
    "alldocs = []\n",
    "for line, tags in zip(data['preprocessed_content'], data['relationship_type']):\n",
    "    tokens = gensim.utils.to_unicode(line).split()\n",
    "    alldocs.append(TaggedDocument(words, tags))\n",
    "\n",
    "# train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "# test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "# print('%d docs: %d train, %d test' % (len(alldocs), len(train_docs), len(test_docs)))\n",
    "\n",
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)\n",
    "\n",
    "print(len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t20) vocabulary scanned & state initialized\n",
      "CPU times: user 6min 14s, sys: 1.27 s, total: 6min 16s\n",
      "Wall time: 6min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build models\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "# cores = multiprocessing.cpu_count()\n",
    "cores = 20\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t20)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    256\u001b[0m                 \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mraw_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mraw_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    225\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    226\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             report_delay=report_delay)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs) # Adjust epochs in model \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative word vector evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'smashing' (20 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t20)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t20)</th></tr><tr><td>[('mutations', 0.39711183309555054),<br>\n",
       "('stealthily', 0.39297765493392944),<br>\n",
       "('fondest', 0.38682976365089417),<br>\n",
       "('surfer', 0.3744651675224304),<br>\n",
       "('discouraging', 0.35691094398498535),<br>\n",
       "('dotted', 0.35466301441192627),<br>\n",
       "('uahan', 0.3428993225097656),<br>\n",
       "('rty', 0.342846542596817),<br>\n",
       "('thirds', 0.34089499711990356),<br>\n",
       "('shielding', 0.3404473662376404),<br>\n",
       "('authentic', 0.3356667757034302),<br>\n",
       "('lent', 0.3335157632827759),<br>\n",
       "('worshipped', 0.3328370749950409),<br>\n",
       "('plunged', 0.33239153027534485),<br>\n",
       "('engaging', 0.33150768280029297),<br>\n",
       "('slinks', 0.3224559724330902),<br>\n",
       "('inform', 0.32213205099105835),<br>\n",
       "('preps', 0.32165539264678955),<br>\n",
       "('acquaintances', 0.3209088146686554),<br>\n",
       "('claudia', 0.318649023771286)]</td><td>[('mutations', 0.39711180329322815),<br>\n",
       "('stealthily', 0.39297762513160706),<br>\n",
       "('fondest', 0.38682979345321655),<br>\n",
       "('surfer', 0.37446513772010803),<br>\n",
       "('discouraging', 0.3569110035896301),<br>\n",
       "('dotted', 0.35466307401657104),<br>\n",
       "('uahan', 0.3428993225097656),<br>\n",
       "('rty', 0.34284651279449463),<br>\n",
       "('thirds', 0.34089499711990356),<br>\n",
       "('shielding', 0.34044742584228516),<br>\n",
       "('lent', 0.3335157632827759),<br>\n",
       "('worshipped', 0.3328370749950409),<br>\n",
       "('plunged', 0.33239153027534485),<br>\n",
       "('engaging', 0.33150771260261536),<br>\n",
       "('slinks', 0.3224559724330902),<br>\n",
       "('inform', 0.32213202118873596),<br>\n",
       "('acquaintances', 0.3209088444709778),<br>\n",
       "('coffee', 0.31975361704826355),<br>\n",
       "('claudia', 0.3186489939689636),<br>\n",
       "('advancing', 0.3178688883781433)]</td><td>[('hugh', 0.40148839354515076),<br>\n",
       "('mutations', 0.39711180329322815),<br>\n",
       "('stealthily', 0.39297765493392944),<br>\n",
       "('fondest', 0.38682976365089417),<br>\n",
       "('surfer', 0.3744651675224304),<br>\n",
       "('orders', 0.35907721519470215),<br>\n",
       "('discouraging', 0.35691094398498535),<br>\n",
       "('dotted', 0.35466301441192627),<br>\n",
       "('uahan', 0.3428993225097656),<br>\n",
       "('rty', 0.342846542596817),<br>\n",
       "('thirds', 0.34089499711990356),<br>\n",
       "('shielding', 0.3404473662376404),<br>\n",
       "('uninhibited', 0.33743056654930115),<br>\n",
       "('lent', 0.3335157632827759),<br>\n",
       "('natural', 0.33288058638572693),<br>\n",
       "('worshipped', 0.3328370749950409),<br>\n",
       "('plunged', 0.33239153027534485),<br>\n",
       "('engaging', 0.33150768280029297),<br>\n",
       "('human', 0.329820841550827),<br>\n",
       "('slinks', 0.3224559724330902)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(simple_models[0].wv.index2word)\n",
    "    if simple_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "        \n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "# word = 'difficult'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in simple_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in simple_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gensim Doc2Vec with AO3 data (document = paragraph)\n",
    "From https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "fpath = '/usr2/mamille2/fanfiction-project/data/ao3/friends/friends_paragraphs.txt'\n",
    "test_fraction = 0.1\n",
    "total_docs = 74199\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags split')\n",
    "\n",
    "alldocs = []\n",
    "with smart_open(fpath, 'rb', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # tags are labels; 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test'][line_no // int(total_docs * (1-test_fraction))]\n",
    "        alldocs.append(TaggedDocument(words, tags, split))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "print('%d docs: %d train, %d test' % (len(alldocs), len(train_docs), len(test_docs)))\n",
    "\n",
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t20) vocabulary scanned & state initialized\n",
      "CPU times: user 9.44 s, sys: 664 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build models\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "# cores = multiprocessing.cpu_count()\n",
    "cores = 20\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t20)\n",
      "CPU times: user 3min 16s, sys: 18.4 s, total: 3min 34s\n",
      "Wall time: 1min 51s\n",
      "\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)\n",
      "CPU times: user 5min 17s, sys: 55.2 s, total: 6min 12s\n",
      "Wall time: 2min 47s\n",
      "\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc2,t20)\n",
      "CPU times: user 8min 27s, sys: 25.1 s, total: 8min 52s\n",
      "Wall time: 2min 13s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'murmur' (13 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t20)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t20)</th></tr><tr><td>[('unsuspecting', 0.39443328976631165),<br>\n",
       "('cellphone', 0.3796854019165039),<br>\n",
       "('iffy', 0.372952401638031),<br>\n",
       "('excess', 0.3608635663986206),<br>\n",
       "('redford', 0.3578962981700897),<br>\n",
       "('hints', 0.3546637296676636),<br>\n",
       "('activated', 0.34775805473327637),<br>\n",
       "('happenings', 0.3403438925743103),<br>\n",
       "('scraping', 0.33970412611961365),<br>\n",
       "('with', 0.3392728269100189),<br>\n",
       "('dialled', 0.33752650022506714),<br>\n",
       "('flavor', 0.33679360151290894),<br>\n",
       "('è', 0.3287610709667206),<br>\n",
       "('on-', 0.32636886835098267),<br>\n",
       "('kämpfer', 0.32536137104034424),<br>\n",
       "('cowardice', 0.3245278596878052),<br>\n",
       "('naturedly', 0.3223896622657776),<br>\n",
       "('employers', 0.32194066047668457),<br>\n",
       "('absolution', 0.31825727224349976),<br>\n",
       "('valasus', 0.3180891275405884)]</td><td>[('whisper', 0.6813105344772339),<br>\n",
       "('growl', 0.5926134586334229),<br>\n",
       "('coward', 0.5922876000404358),<br>\n",
       "('moment', 0.573410153388977),<br>\n",
       "('burglar', 0.5642147064208984),<br>\n",
       "('mover', 0.5640881657600403),<br>\n",
       "('teenager', 0.5636329054832458),<br>\n",
       "('minute', 0.5632354021072388),<br>\n",
       "('halt', 0.5546646118164062),<br>\n",
       "('sob', 0.5538667440414429),<br>\n",
       "('skleničku', 0.5502578616142273),<br>\n",
       "('continent', 0.5448827743530273),<br>\n",
       "('huff', 0.5434749722480774),<br>\n",
       "('mockery', 0.5430886745452881),<br>\n",
       "('word', 0.5400181412696838),<br>\n",
       "('sniffle', 0.5378166437149048),<br>\n",
       "('click', 0.5377556681632996),<br>\n",
       "('volver', 0.5347557663917542),<br>\n",
       "('participant', 0.5343178510665894),<br>\n",
       "('bit', 0.5334388613700867)]</td><td>[('enrol', 0.6278350949287415),<br>\n",
       "('twig', 0.6177855730056763),<br>\n",
       "('snort', 0.6093369126319885),<br>\n",
       "('halt', 0.6067569255828857),<br>\n",
       "('fight.’', 0.5970606803894043),<br>\n",
       "('revolt', 0.591293215751648),<br>\n",
       "('yawn', 0.5861109495162964),<br>\n",
       "('sob', 0.5846139192581177),<br>\n",
       "('wedge', 0.5762349963188171),<br>\n",
       "('swerve', 0.5714218616485596),<br>\n",
       "('nestle', 0.5631072521209717),<br>\n",
       "('caress', 0.5629807710647583),<br>\n",
       "('miscommunication', 0.5629664659500122),<br>\n",
       "('sooth', 0.5585294365882874),<br>\n",
       "('rook', 0.5577542781829834),<br>\n",
       "('puddle', 0.5549665093421936),<br>\n",
       "('connoisseur', 0.5526505708694458),<br>\n",
       "('tweak', 0.5496055483818054),<br>\n",
       "('yuki', 0.547900378704071),<br>\n",
       "('swat', 0.546383261680603)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(simple_models[0].wv.index2word)\n",
    "    if simple_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "        \n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "# word = 'spoilt'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in simple_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in simple_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (37520): «alright ... \" the blond stares at nico before suddenly inhaling sharply . \" well , there you go ... \"»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20):\n",
      "\n",
      "MOST (74142, 0.7427983283996582): «nodded again . \" okay . \"»\n",
      "\n",
      "MEDIAN (32786, 0.407391756772995): «closes the door behind her , and joey shakes his head . thank god rachel did n’t notice anything different about him .»\n",
      "\n",
      "LEAST (40699, -0.3952307403087616): «_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gensim Doc2Vec with provided data\n",
    "From https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, alldata-id.txt is available for next steps.\n",
      "CPU times: user 240 ms, sys: 40 ms, total: 280 ms\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "\n",
    "dirname = 'aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with smart_open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg', 'train/unsup']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline = \"\\n\".encode(\"utf-8\")\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n",
      "CPU times: user 8.72 s, sys: 1.27 s, total: 9.99 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []\n",
    "with smart_open('aclImdb/alldata-id.txt', 'rb', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(alldocs), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t20) vocabulary scanned & state initialized\n",
      "CPU times: user 40.8 s, sys: 4.29 s, total: 45.1 s\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "# cores = multiprocessing.cpu_count()\n",
    "cores = 20\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t20)\n",
      "CPU times: user 25min 13s, sys: 58 s, total: 26min 11s\n",
      "Wall time: 6min 20s\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)\n",
      "CPU times: user 53min 28s, sys: 2min 32s, total: 56min 1s\n",
      "Wall time: 9min 54s\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc2,t20)\n",
      "CPU times: user 2h 49s, sys: 2min 42s, total: 2h 3min 31s\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'spoilt' (97 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t20)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t20)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t20)</th></tr><tr><td>[('ballet', 0.4222918152809143),<br>\n",
       "('65-minute', 0.41812703013420105),<br>\n",
       "('nit-wit', 0.38984495401382446),<br>\n",
       "('action-comedy', 0.3879348337650299),<br>\n",
       "('pimped', 0.37372422218322754),<br>\n",
       "('matara', 0.3736003637313843),<br>\n",
       "('brownstone', 0.37183868885040283),<br>\n",
       "(\"dick's\", 0.36779266595840454),<br>\n",
       "(\"rodger's\", 0.367227703332901),<br>\n",
       "('verano', 0.3622084856033325),<br>\n",
       "('consummately', 0.3592488169670105),<br>\n",
       "(\"macready's\", 0.3578265309333801),<br>\n",
       "('*such*', 0.3577822148799896),<br>\n",
       "('krassin', 0.3576417863368988),<br>\n",
       "('risks', 0.35670018196105957),<br>\n",
       "('callings', 0.35649171471595764),<br>\n",
       "('aic', 0.35543423891067505),<br>\n",
       "('distracts', 0.3541957139968872),<br>\n",
       "(\"freaks'\", 0.35217738151550293),<br>\n",
       "('ronreaco', 0.35198500752449036)]</td><td>[('spoiled', 0.6510179042816162),<br>\n",
       "('undermined', 0.5426546335220337),<br>\n",
       "('dominated', 0.5321130752563477),<br>\n",
       "('ruined', 0.5287238955497742),<br>\n",
       "('over-shadowed', 0.5230126976966858),<br>\n",
       "('marred', 0.5212410688400269),<br>\n",
       "('replaced', 0.5141274333000183),<br>\n",
       "('followed', 0.4941771924495697),<br>\n",
       "('siring', 0.49131008982658386),<br>\n",
       "('rejected', 0.4876922070980072),<br>\n",
       "('dwarfed', 0.48443150520324707),<br>\n",
       "('unencumbered', 0.4830629825592041),<br>\n",
       "('snatched', 0.479458749294281),<br>\n",
       "('populated', 0.47769853472709656),<br>\n",
       "('dazzled', 0.4664037823677063),<br>\n",
       "('inhabited', 0.46631473302841187),<br>\n",
       "('entranced', 0.4659886360168457),<br>\n",
       "('surrounded', 0.46457332372665405),<br>\n",
       "('bolstered', 0.463314026594162),<br>\n",
       "('saved', 0.46262553334236145)]</td><td>[('spoiled', 0.6605638265609741),<br>\n",
       "('ruined', 0.5010989904403687),<br>\n",
       "('disturbed', 0.49336788058280945),<br>\n",
       "('plagued', 0.48810648918151855),<br>\n",
       "('wooed', 0.48625805974006653),<br>\n",
       "('dampened', 0.4841140806674957),<br>\n",
       "('lightened', 0.4838625490665436),<br>\n",
       "('shaken', 0.47890323400497437),<br>\n",
       "('troubled', 0.4785526990890503),<br>\n",
       "('torpedoed', 0.47636711597442627),<br>\n",
       "('racked', 0.4725381135940552),<br>\n",
       "('maltreated', 0.4714895784854889),<br>\n",
       "('wounded', 0.46761342883110046),<br>\n",
       "('lessened', 0.4648064076900482),<br>\n",
       "('drugged', 0.46392497420310974),<br>\n",
       "('stimulated', 0.46230459213256836),<br>\n",
       "('breakable', 0.46102404594421387),<br>\n",
       "('saved', 0.4606311619281769),<br>\n",
       "('cuckolded', 0.4579746723175049),<br>\n",
       "('willed', 0.455874502658844)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# pick a random word with a suitable number of occurences\n",
    "# while True:\n",
    "#     word = random.choice(simple_models[0].wv.index2word)\n",
    "#     if simple_models[0].wv.vocab[word].count > 10:\n",
    "#         break\n",
    "        \n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "word = 'spoilt'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in simple_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in simple_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
